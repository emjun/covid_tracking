{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save data locally\n",
    "states_url = 'https://covidtracking.com/api/states.csv'\n",
    "states_local = 'states.csv'\n",
    "states_local_clean = 'states_clean.csv'\n",
    "\n",
    "# https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3\n",
    "with urllib.request.urlopen(states_url) as response, open(states_local, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we define \"tested\"?\n",
    "Two options: \n",
    "1. tested_1 = positive + negative\n",
    "2. tested_2 = positive + negative + pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Definition of TESTED\n",
    "AKA: 1. tested_1 = positive + negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state  positive  positiveScore  negativeScore  negativeRegularScore  \\\n",
      "1     AL     138.0            1.0            1.0                   0.0   \n",
      "2     AR     165.0            1.0            1.0                   1.0   \n",
      "3     AZ     152.0            1.0            1.0                   1.0   \n",
      "4     CA    1536.0            1.0            1.0                   1.0   \n",
      "5     CO     475.0            1.0            1.0                   1.0   \n",
      "6     CT     223.0            1.0            1.0                   1.0   \n",
      "7     DC      98.0            1.0            1.0                   1.0   \n",
      "8     DE      56.0            1.0            1.0                   0.0   \n",
      "9     FL     830.0            1.0            1.0                   1.0   \n",
      "10    GA     600.0            1.0            1.0                   1.0   \n",
      "13    ID      42.0            1.0            1.0                   1.0   \n",
      "14    IL    1058.0            1.0            1.0                   1.0   \n",
      "15    IN     201.0            1.0            1.0                   1.0   \n",
      "16    KS      64.0            1.0            1.0                   1.0   \n",
      "17    KY      99.0            1.0            1.0                   1.0   \n",
      "18    LA     837.0            1.0            1.0                   1.0   \n",
      "19    MA     646.0            1.0            1.0                   0.0   \n",
      "20    MD     244.0            1.0            0.0                   0.0   \n",
      "22    MI    1035.0            1.0            0.0                   0.0   \n",
      "23    MN     169.0            1.0            1.0                   1.0   \n",
      "24    MO      90.0            1.0            1.0                   0.0   \n",
      "25    MS     207.0            1.0            1.0                   1.0   \n",
      "27    NC     255.0            1.0            1.0                   1.0   \n",
      "28    ND      28.0            1.0            1.0                   1.0   \n",
      "29    NE      48.0            1.0            1.0                   1.0   \n",
      "31    NJ    1914.0            1.0            1.0                   1.0   \n",
      "33    NV     190.0            1.0            1.0                   1.0   \n",
      "34    NY   15168.0            1.0            1.0                   0.0   \n",
      "35    OH     351.0            1.0            0.0                   0.0   \n",
      "36    OK      67.0            1.0            1.0                   1.0   \n",
      "37    OR     161.0            1.0            1.0                   1.0   \n",
      "38    PA     479.0            1.0            1.0                   1.0   \n",
      "40    SC     195.0            1.0            1.0                   1.0   \n",
      "41    SD      21.0            1.0            1.0                   1.0   \n",
      "43    TX     334.0            1.0            1.0                   1.0   \n",
      "44    UT     181.0            1.0            1.0                   1.0   \n",
      "45    VA     219.0            1.0            1.0                   1.0   \n",
      "46    VT      52.0            1.0            1.0                   1.0   \n",
      "47    WA    1793.0            1.0            1.0                   1.0   \n",
      "48    WI     385.0            1.0            1.0                   1.0   \n",
      "49    WV      12.0            1.0            1.0                   1.0   \n",
      "53    GU      27.0            NaN            NaN                   NaN   \n",
      "\n",
      "    commercialScore grade  score  negative  pending  hospitalized  death  \\\n",
      "1               0.0     C    2.0    1464.0      NaN           NaN    0.0   \n",
      "2               1.0     A    4.0     711.0    119.0          13.0    0.0   \n",
      "3               0.0     B    3.0     282.0     87.0           NaN    2.0   \n",
      "4               0.0     B    3.0   11304.0      NaN           NaN   27.0   \n",
      "5               1.0     A    4.0    4075.0      NaN          49.0    5.0   \n",
      "6               1.0     A    4.0    2877.0      NaN          43.0    5.0   \n",
      "7               1.0     A    4.0     957.0      NaN           NaN    1.0   \n",
      "8               0.0     C    2.0      36.0      NaN           NaN    0.0   \n",
      "9               1.0     A    4.0    7990.0    963.0         185.0   13.0   \n",
      "10              1.0     A    4.0    3420.0      NaN           NaN   23.0   \n",
      "13              1.0     A    4.0    1175.0      NaN           NaN    0.0   \n",
      "14              1.0     A    4.0    7271.0      NaN           NaN    9.0   \n",
      "15              0.0     B    3.0    1293.0      NaN           1.0    6.0   \n",
      "16              0.0     B    3.0     417.0      NaN           NaN    2.0   \n",
      "17              1.0     A    4.0    1472.0      NaN           NaN    3.0   \n",
      "18              1.0     B    4.0    2661.0      NaN           NaN   20.0   \n",
      "19              1.0     B    3.0    5482.0      NaN          71.0    5.0   \n",
      "20              0.0     D    1.0      94.0      NaN           NaN    3.0   \n",
      "22              0.0     D    1.0    2069.0      NaN           NaN    8.0   \n",
      "23              1.0     A    4.0    4511.0      NaN          12.0    1.0   \n",
      "24              0.0     C    2.0     369.0      NaN           NaN    3.0   \n",
      "25              0.0     B    3.0    1114.0      NaN          33.0    1.0   \n",
      "27              1.0     A    4.0    6183.0      NaN           NaN    0.0   \n",
      "28              0.0     B    3.0    1260.0      NaN           3.0    0.0   \n",
      "29              1.0     A    4.0     356.0      NaN           NaN    0.0   \n",
      "31              0.0     B    3.0     327.0     49.0           NaN   20.0   \n",
      "33              1.0     A    4.0    2448.0      NaN           NaN    2.0   \n",
      "34              1.0     B    3.0   46233.0      NaN        1974.0  114.0   \n",
      "35              0.0     D    1.0     140.0      NaN          83.0    3.0   \n",
      "36              1.0     A    4.0     669.0    102.0          11.0    2.0   \n",
      "37              1.0     A    4.0    2864.0      NaN          43.0    4.0   \n",
      "38              1.0     A    4.0    4964.0      NaN           NaN    2.0   \n",
      "40              1.0     A    4.0    1466.0      NaN           NaN    3.0   \n",
      "41              1.0     A    4.0     740.0    277.0           NaN    1.0   \n",
      "43              1.0     A    4.0    8422.0      NaN           NaN    5.0   \n",
      "44              1.0     A    4.0    3508.0      NaN           NaN    1.0   \n",
      "45              1.0     A    4.0    3118.0      NaN          32.0    3.0   \n",
      "46              1.0     A    4.0    1106.0      NaN           NaN    2.0   \n",
      "47              1.0     A    4.0   25328.0      NaN           NaN   94.0   \n",
      "48              1.0     A    4.0    6230.0      NaN           NaN    4.0   \n",
      "49              0.0     B    3.0     385.0      1.0           1.0    0.0   \n",
      "53              NaN   NaN    NaN     126.0      NaN           NaN    1.0   \n",
      "\n",
      "    total lastUpdateEt checkTimeEt          dateModified  \\\n",
      "1    1602   3/22 10:37  3/22 16:00  2020-03-22T14:37:00Z   \n",
      "2     995   3/22 13:45  3/22 15:15  2020-03-22T17:45:00Z   \n",
      "3     521   3/22 00:00  3/22 16:01  2020-03-22T04:00:00Z   \n",
      "4   12840   3/22 13:34  3/22 14:41  2020-03-22T17:34:00Z   \n",
      "5    4550   3/21 18:00  3/22 16:02  2020-03-21T22:00:00Z   \n",
      "6    3100   3/21 19:30  3/22 16:51  2020-03-21T23:30:00Z   \n",
      "7    1055   3/21 19:30  3/22 16:23  2020-03-21T23:30:00Z   \n",
      "8      92   3/22 16:15  3/22 16:52  2020-03-22T20:15:00Z   \n",
      "9    9783   3/22 11:00  3/22 16:26  2020-03-22T15:00:00Z   \n",
      "10   4020   3/22 12:00  3/22 16:28  2020-03-22T16:00:00Z   \n",
      "13   1217   3/21 19:55  3/22 16:32  2020-03-21T23:55:00Z   \n",
      "14   8329   3/22 00:00  3/22 16:34  2020-03-22T04:00:00Z   \n",
      "15   1494   3/13 00:59  3/22 16:35  2020-03-13T04:59:00Z   \n",
      "16    481   3/21 11:00  3/22 16:37  2020-03-21T15:00:00Z   \n",
      "17   1571   3/22 09:00  3/22 16:38  2020-03-22T13:00:00Z   \n",
      "18   3498   3/22 10:00  3/22 16:38  2020-03-22T14:00:00Z   \n",
      "19   6128   3/22 16:00  3/22 16:56  2020-03-22T20:00:00Z   \n",
      "20    338   3/22 10:00  3/22 16:40  2020-03-22T14:00:00Z   \n",
      "22   3104   3/22 15:00  3/22 16:19  2020-03-22T19:00:00Z   \n",
      "23   4680   3/21 12:10  3/22 16:47  2020-03-21T16:10:00Z   \n",
      "24    459   3/21 22:00  3/22 15:38  2020-03-22T02:00:00Z   \n",
      "25   1321   3/22 11:20  3/22 15:30  2020-03-22T15:20:00Z   \n",
      "27   6438   3/22 10:16  3/22 16:44  2020-03-22T14:16:00Z   \n",
      "28   1288   3/22 11:39  3/22 15:41  2020-03-22T15:39:00Z   \n",
      "29    404   3/21 00:00  3/22 16:44  2020-03-21T04:00:00Z   \n",
      "31   2290   3/22 13:30  3/22 16:43  2020-03-22T17:30:00Z   \n",
      "33   2638   3/21 14:00  3/22 16:41  2020-03-21T18:00:00Z   \n",
      "34  61401   3/22 11:00  3/22 15:15  2020-03-22T15:00:00Z   \n",
      "35    491   3/20 14:00  3/22 16:13  2020-03-20T18:00:00Z   \n",
      "36    838   3/22 08:00  3/22 15:38  2020-03-22T12:00:00Z   \n",
      "37   3025   3/22 11:00  3/22 15:53  2020-03-22T15:00:00Z   \n",
      "38   5443   3/22 12:00  3/22 16:14  2020-03-22T16:00:00Z   \n",
      "40   1661   3/22 15:40  3/22 16:10  2020-03-22T19:40:00Z   \n",
      "41   1038   3/22 12:30  3/22 16:11  2020-03-22T16:30:00Z   \n",
      "43   8756   3/21 21:00  3/22 15:23  2020-03-22T01:00:00Z   \n",
      "44   3689   3/22 15:00  3/22 15:40  2020-03-22T19:00:00Z   \n",
      "45   3337   3/21 17:00  3/22 16:51  2020-03-21T21:00:00Z   \n",
      "46   1158   3/22 14:00  3/22 15:19  2020-03-22T18:00:00Z   \n",
      "47  27121   3/21 18:00  3/22 15:21  2020-03-21T22:00:00Z   \n",
      "48   6615   3/22 04:00  3/22 15:34  2020-03-22T08:00:00Z   \n",
      "49    398   3/21 00:00  3/22 15:39  2020-03-21T04:00:00Z   \n",
      "53    153   3/22 07:00  3/22 16:28  2020-03-22T11:00:00Z   \n",
      "\n",
      "             dateChecked  tested_1  \n",
      "1   2020-03-22T20:00:00Z    1602.0  \n",
      "2   2020-03-22T19:15:00Z     876.0  \n",
      "3   2020-03-22T20:01:00Z     434.0  \n",
      "4   2020-03-22T18:41:00Z   12840.0  \n",
      "5   2020-03-22T20:02:00Z    4550.0  \n",
      "6   2020-03-22T20:51:00Z    3100.0  \n",
      "7   2020-03-22T20:23:00Z    1055.0  \n",
      "8   2020-03-22T20:52:00Z      92.0  \n",
      "9   2020-03-22T20:26:00Z    8820.0  \n",
      "10  2020-03-22T20:28:00Z    4020.0  \n",
      "13  2020-03-22T20:32:00Z    1217.0  \n",
      "14  2020-03-22T20:34:00Z    8329.0  \n",
      "15  2020-03-22T20:35:00Z    1494.0  \n",
      "16  2020-03-22T20:37:00Z     481.0  \n",
      "17  2020-03-22T20:38:00Z    1571.0  \n",
      "18  2020-03-22T20:38:00Z    3498.0  \n",
      "19  2020-03-22T20:56:00Z    6128.0  \n",
      "20  2020-03-22T20:40:00Z     338.0  \n",
      "22  2020-03-22T20:19:00Z    3104.0  \n",
      "23  2020-03-22T20:47:00Z    4680.0  \n",
      "24  2020-03-22T19:38:00Z     459.0  \n",
      "25  2020-03-22T19:30:00Z    1321.0  \n",
      "27  2020-03-22T20:44:00Z    6438.0  \n",
      "28  2020-03-22T19:41:00Z    1288.0  \n",
      "29  2020-03-22T20:44:00Z     404.0  \n",
      "31  2020-03-22T20:43:00Z    2241.0  \n",
      "33  2020-03-22T20:41:00Z    2638.0  \n",
      "34  2020-03-22T19:15:00Z   61401.0  \n",
      "35  2020-03-22T20:13:00Z     491.0  \n",
      "36  2020-03-22T19:38:00Z     736.0  \n",
      "37  2020-03-22T19:53:00Z    3025.0  \n",
      "38  2020-03-22T20:14:00Z    5443.0  \n",
      "40  2020-03-22T20:10:00Z    1661.0  \n",
      "41  2020-03-22T20:11:00Z     761.0  \n",
      "43  2020-03-22T19:23:00Z    8756.0  \n",
      "44  2020-03-22T19:40:00Z    3689.0  \n",
      "45  2020-03-22T20:51:00Z    3337.0  \n",
      "46  2020-03-22T19:19:00Z    1158.0  \n",
      "47  2020-03-22T19:21:00Z   27121.0  \n",
      "48  2020-03-22T19:34:00Z    6615.0  \n",
      "49  2020-03-22T19:39:00Z     397.0  \n",
      "53  2020-03-22T20:28:00Z     153.0  \n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning/wrangling.\n",
    "# Note: This is a workaround. A slightly newer version of Tea supports data \"import\" from Pandas.\n",
    "\n",
    "# CLEAN DATA and REMOVE any MISSING data from the dataframe.\n",
    "# This is necessary because Tea assumes that the dataset is cleaned (no missing values).\n",
    "df = pd.read_csv(states_local)\n",
    "\n",
    "df = df.dropna(subset=['positive', 'negative', 'death']) # These are the columns we care about/want to make sure there are no missing values\n",
    "\n",
    "# CREATE a NEW COLUMN/VARIABLE for the total number of tested cases. \n",
    "print(f\"Sample size: {len(df)}\")\n",
    "df['tested_1'] = df['positive'] + df['negative']\n",
    "print(df)\n",
    "df.to_csv(states_local_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: Load data from URL. Could also load data from local copy.\n",
    "tea.data(states_local_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: Specify variables of interest in dataset\n",
    "variables = [\n",
    "    {\n",
    "        'name' : 'positive',\n",
    "        'data type' : 'ratio'   # Options: 'nominal', 'ordinal', 'interval', 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'negative',\n",
    "        'data type' : 'ratio'   # Options: 'nominal', 'ordinal', 'interval', 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'pending',\n",
    "        'data type' : 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'tested',\n",
    "        'data type' : 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'death',\n",
    "        'data type' : 'ratio'\n",
    "    }\n",
    "]\n",
    "\n",
    "tea.define_variables(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: [OPTIONAL] We don't have any assumptions, so we can skip this step. \n",
    "assumptions = {\n",
    "    'Type I (False Positive) Error Rate': 0.05\n",
    "}\n",
    "\n",
    "tea.assume(assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: Specify experimental design\n",
    "experimental_design = {\n",
    "                        'study type': 'observational study',   # 'study type' could be 'experiment'\n",
    "                        'contributor variables': ['positive', 'negative', 'tested'],   # 'experiment's have 'independent variables'\n",
    "                        'outcome variables': 'death',   # 'experiment's have 'dependent variables'\n",
    "                    }\n",
    "tea.define_study_design(experimental_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently considering pearson_corr\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous.\n",
      "Property holds.\n",
      "Testing assumption: is_normal.\n",
      "Property FAILS\n",
      "Testing assumption: is_normal.\n",
      "\n",
      "Currently considering kendalltau_corr\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "\n",
      "Currently considering spearman_corr\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "\n",
      "Currently considering pointbiserial_corr_a\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous.\n",
      "Property holds.\n",
      "Testing assumption: is_normal.\n",
      "Property FAILS\n",
      "Testing assumption: is_categorical.\n",
      "Testing assumption: has_two_categories.\n",
      "Testing assumption: has_equal_variance.\n",
      "\n",
      "Currently considering pointbiserial_corr_b\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering students_t\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: has_one_x.\n",
      "Property holds.\n",
      "Testing assumption: has_one_y.\n",
      "Property holds.\n",
      "Testing assumption: has_independent_observations.\n",
      "Property holds.\n",
      "Testing assumption: is_categorical.\n",
      "Property FAILS\n",
      "Testing assumption: has_two_categories.\n",
      "Testing assumption: is_continuous.\n",
      "Testing assumption: has_equal_variance.\n",
      "Testing assumption: is_groups_normal.\n",
      "\n",
      "Currently considering welchs_t\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering mannwhitney_u\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering paired_students_t\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering wilcoxon_signed_rank\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering chi_square\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering fishers_exact\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering f_test\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering kruskall_wallis\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering rm_one_way_anova\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering friedman\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering factorial_ANOVA\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "--------------\n",
      "Test: kendalltau_corr\n",
      "***Test assumptions:\n",
      "None\n",
      "\n",
      "***Test results:\n",
      "name = Kendall Tau Correlation\n",
      "test_statistic = 0.45056355688958294\n",
      "p_value = 0.16658764010038207\n",
      "\n",
      "Test: spearman_corr\n",
      "***Test assumptions:\n",
      "None\n",
      "\n",
      "***Test results:\n",
      "name = Spearman R Correlation\n",
      "test_statistic = 0.6182840223353118\n",
      "p_value = 0.13889518793753775\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Results:\n",
       "--------------\n",
       "Test: kendalltau_corr\n",
       "***Test assumptions:\n",
       "None\n",
       "\n",
       "***Test results:\n",
       "name = Kendall Tau Correlation\n",
       "test_statistic = 0.45056355688958294\n",
       "p_value = 0.16658764010038207\n",
       "\n",
       "Test: spearman_corr\n",
       "***Test assumptions:\n",
       "None\n",
       "\n",
       "***Test results:\n",
       "name = Spearman R Correlation\n",
       "test_statistic = 0.6182840223353118\n",
       "p_value = 0.13889518793753775"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tea: State and test hypothesis\n",
    "tea.hypothesize(['tested', 'death'], ['tested ~ death'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result: There seems to be moderate positive statistically significant relationship between how many people are tested and how many deaths there are. \n",
    "# In other words, in states that test more people, more people die. This doesn't suggest causality!\n",
    "\n",
    "# BUG!: Tea doesn't seem to be outputting the interpretation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Definition of TESTED\n",
    "AKA: 1. tested_2 = positive + negative + pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state  positive  positiveScore  negativeScore  negativeRegularScore  \\\n",
      "2     AR     165.0            1.0            1.0                   1.0   \n",
      "3     AZ     152.0            1.0            1.0                   1.0   \n",
      "9     FL     830.0            1.0            1.0                   1.0   \n",
      "31    NJ    1914.0            1.0            1.0                   1.0   \n",
      "36    OK      67.0            1.0            1.0                   1.0   \n",
      "41    SD      21.0            1.0            1.0                   1.0   \n",
      "49    WV      12.0            1.0            1.0                   1.0   \n",
      "\n",
      "    commercialScore grade  score  negative  pending  hospitalized  death  \\\n",
      "2               1.0     A    4.0     711.0    119.0          13.0    0.0   \n",
      "3               0.0     B    3.0     282.0     87.0           NaN    2.0   \n",
      "9               1.0     A    4.0    7990.0    963.0         185.0   13.0   \n",
      "31              0.0     B    3.0     327.0     49.0           NaN   20.0   \n",
      "36              1.0     A    4.0     669.0    102.0          11.0    2.0   \n",
      "41              1.0     A    4.0     740.0    277.0           NaN    1.0   \n",
      "49              0.0     B    3.0     385.0      1.0           1.0    0.0   \n",
      "\n",
      "    total lastUpdateEt checkTimeEt          dateModified  \\\n",
      "2     995   3/22 13:45  3/22 15:15  2020-03-22T17:45:00Z   \n",
      "3     521   3/22 00:00  3/22 16:01  2020-03-22T04:00:00Z   \n",
      "9    9783   3/22 11:00  3/22 16:26  2020-03-22T15:00:00Z   \n",
      "31   2290   3/22 13:30  3/22 16:43  2020-03-22T17:30:00Z   \n",
      "36    838   3/22 08:00  3/22 15:38  2020-03-22T12:00:00Z   \n",
      "41   1038   3/22 12:30  3/22 16:11  2020-03-22T16:30:00Z   \n",
      "49    398   3/21 00:00  3/22 15:39  2020-03-21T04:00:00Z   \n",
      "\n",
      "             dateChecked  tested  \n",
      "2   2020-03-22T19:15:00Z   995.0  \n",
      "3   2020-03-22T20:01:00Z   521.0  \n",
      "9   2020-03-22T20:26:00Z  9783.0  \n",
      "31  2020-03-22T20:43:00Z  2290.0  \n",
      "36  2020-03-22T19:38:00Z   838.0  \n",
      "41  2020-03-22T20:11:00Z  1038.0  \n",
      "49  2020-03-22T19:39:00Z   398.0  \n"
     ]
    }
   ],
   "source": [
    "# Data cleaning/wrangling.\n",
    "# Note: This is a workaround. A slightly newer version of Tea supports data \"import\" from Pandas.\n",
    "\n",
    "# CLEAN DATA and REMOVE any MISSING data from the dataframe.\n",
    "# This is necessary because Tea assumes that the dataset is cleaned (no missing values).\n",
    "df = pd.read_csv(states_local)\n",
    "\n",
    "df = df.dropna(subset=['positive', 'negative', 'death']) # These are the columns we care about/want to make sure there are no missing values\n",
    "\n",
    "# CREATE a NEW COLUMN/VARIABLE for the total number of tested cases. \n",
    "print(f\"Sample size: {len(df)}\")\n",
    "df['tested_2'] = df['positive'] + df['negative'] + df['pending']\n",
    "print(df)\n",
    "df.to_csv(states_local_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: Load data from URL. Could also load data from local copy.\n",
    "tea.data(states_local_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: Specify variables of interest in dataset\n",
    "variables = [\n",
    "    {\n",
    "        'name' : 'positive',\n",
    "        'data type' : 'ratio'   # Options: 'nominal', 'ordinal', 'interval', 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'negative',\n",
    "        'data type' : 'ratio'   # Options: 'nominal', 'ordinal', 'interval', 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'pending',\n",
    "        'data type' : 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'tested',\n",
    "        'data type' : 'ratio'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'death',\n",
    "        'data type' : 'ratio'\n",
    "    }\n",
    "]\n",
    "\n",
    "tea.define_variables(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: [OPTIONAL] We don't have any assumptions, so we can skip this step. \n",
    "assumptions = {\n",
    "    'Type I (False Positive) Error Rate': 0.05\n",
    "}\n",
    "\n",
    "tea.assume(assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tea: Specify experimental design\n",
    "experimental_design = {\n",
    "                        'study type': 'observational study',   # 'study type' could be 'experiment'\n",
    "                        'contributor variables': ['positive', 'negative', 'tested'],   # 'experiment's have 'independent variables'\n",
    "                        'outcome variables': 'death',   # 'experiment's have 'dependent variables'\n",
    "                    }\n",
    "tea.define_study_design(experimental_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently considering pearson_corr\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous.\n",
      "Property holds.\n",
      "Testing assumption: is_normal.\n",
      "Property FAILS\n",
      "Testing assumption: is_normal.\n",
      "\n",
      "Currently considering kendalltau_corr\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "\n",
      "Currently considering spearman_corr\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous_or_ordinal.\n",
      "Property holds.\n",
      "\n",
      "Currently considering pointbiserial_corr_a\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: is_continuous.\n",
      "Property holds.\n",
      "Testing assumption: is_normal.\n",
      "Property FAILS\n",
      "Testing assumption: is_categorical.\n",
      "Testing assumption: has_two_categories.\n",
      "Testing assumption: has_equal_variance.\n",
      "\n",
      "Currently considering pointbiserial_corr_b\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering students_t\n",
      "Testing assumption: is_bivariate.\n",
      "Property holds.\n",
      "Testing assumption: has_one_x.\n",
      "Property holds.\n",
      "Testing assumption: has_one_y.\n",
      "Property holds.\n",
      "Testing assumption: has_independent_observations.\n",
      "Property holds.\n",
      "Testing assumption: is_categorical.\n",
      "Property FAILS\n",
      "Testing assumption: has_two_categories.\n",
      "Testing assumption: is_continuous.\n",
      "Testing assumption: has_equal_variance.\n",
      "Testing assumption: is_groups_normal.\n",
      "\n",
      "Currently considering welchs_t\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering mannwhitney_u\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering paired_students_t\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering wilcoxon_signed_rank\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering chi_square\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering fishers_exact\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering f_test\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering kruskall_wallis\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering rm_one_way_anova\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering friedman\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "Currently considering factorial_ANOVA\n",
      "Test is unsat.\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "--------------\n",
      "Test: kendalltau_corr\n",
      "***Test assumptions:\n",
      "None\n",
      "\n",
      "***Test results:\n",
      "name = Kendall Tau Correlation\n",
      "test_statistic = 0.45056355688958294\n",
      "p_value = 0.16658764010038207\n",
      "\n",
      "Test: spearman_corr\n",
      "***Test assumptions:\n",
      "None\n",
      "\n",
      "***Test results:\n",
      "name = Spearman R Correlation\n",
      "test_statistic = 0.6182840223353118\n",
      "p_value = 0.13889518793753775\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Results:\n",
       "--------------\n",
       "Test: kendalltau_corr\n",
       "***Test assumptions:\n",
       "None\n",
       "\n",
       "***Test results:\n",
       "name = Kendall Tau Correlation\n",
       "test_statistic = 0.45056355688958294\n",
       "p_value = 0.16658764010038207\n",
       "\n",
       "Test: spearman_corr\n",
       "***Test assumptions:\n",
       "None\n",
       "\n",
       "***Test results:\n",
       "name = Spearman R Correlation\n",
       "test_statistic = 0.6182840223353118\n",
       "p_value = 0.13889518793753775"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tea: State and test hypothesis\n",
    "tea.hypothesize(['tested', 'death'], ['tested ~ death'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
